\chapter{Conclusion}\label{cap:conclusion}

\section{Summary of Findings}

This work presented both sequential and parallel implementations of the \textit{Sieve of Eratosthenes} algorithm, developed in the C programming language using the Message Passing Interface (MPI).  
The project aimed to evaluate the impact of distributed parallelism on execution performance, scalability, and computational efficiency when applied to a classical algorithm of number theory.  

The sequential version served as a baseline for correctness and performance comparison, while the parallel version adopted the Master–Slave model to distribute the sieving workload among multiple processes.  
The modular design of the codebase, separating MPI orchestration, sieve computation, and utility functions, allowed for a clearer implementation and simplified maintenance, ensuring readability and reusability for educational purposes.

Experimental results demonstrated that the MPI-based version provides consistent and measurable improvements in performance for medium and large workloads.  
Although the smallest test case ($N = 10^6$) exhibited a slight performance drop with only two processes—an expected result given the communication overhead of small tasks—the overall trend showed strong scalability.  
Even for this input size, the runtime decreased substantially when using more processes, reaching a speedup of $S = 2.47$ with ten processes.  
These results confirm that as the problem size increases, the ratio of computation to communication becomes increasingly favorable, allowing the algorithm to scale efficiently.

\section{Contributions}

Beyond achieving the main objective of accelerating the sieve algorithm, this project contributed the following:

\begin{itemize}
    \item A modular, maintainable MPI implementation of the \textit{Sieve of Eratosthenes}, organized into distinct components for orchestration, computation, and utilities;
    \item A complete performance evaluation comparing sequential and parallel versions across multiple input sizes and process counts;
    \item An empirical demonstration of how MPI parallelism scales with problem size and how communication overhead affects smaller workloads;
    \item Educational value through a clean, documented example illustrating key MPI concepts such as broadcast, point-to-point communication, and workload distribution.
\end{itemize}

The results collectively validate that MPI parallelization is both practical and efficient for this type of problem, providing measurable speedups while maintaining algorithmic correctness.

\section{Future Work}

Although the current implementation achieves significant performance improvements, several opportunities exist for further optimization and exploration:

\begin{itemize}
    \item \textbf{Hybrid Parallelization:} Incorporating OpenMP for shared-memory parallelism within each MPI process could reduce communication overhead and better exploit multicore systems.
    \item \textbf{Dynamic Load Balancing:} Adopting adaptive task distribution strategies could improve efficiency in heterogeneous environments or when process workloads are uneven.
\end{itemize}

\section{Final Considerations}

The project successfully demonstrated the advantages and trade-offs of parallel computing using MPI.  
The results reaffirm a central principle of high-performance computing: **parallel efficiency grows with problem size**.  
For larger workloads, the MPI approach clearly outperforms its sequential counterpart, validating the relevance of distributed computing techniques for scalable algorithmic solutions.

This study reinforces the educational and practical value of applying MPI in academic and research contexts, offering both a conceptual and empirical foundation for future explorations in high-performance computing.