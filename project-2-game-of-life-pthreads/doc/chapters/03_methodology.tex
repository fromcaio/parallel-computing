\chapter{Methodology}\label{cap:methodology}

\section{Development Environment}

The software was developed in C11, ensuring compliance with modern standards while maintaining low-level memory control. The compilation uses \texttt{gcc} with the \texttt{-pthread} flag to enable POSIX threads support.

\subsection{Hardware Specifications}
The experiments were conducted on a single-socket workstation with the following configuration:
\begin{itemize}
    \item \textbf{Processor}: Intel Xeon E5-2650 (10 physical cores, 20 hardware threads via SMT).
    \item \textbf{Memory}: 32\,GB DDR4 RAM (ample to hold the largest $5000 \times 5000$ grids and associated buffers).
    \item \textbf{Operating System}: Pop!\_OS 22.04 LTS (64-bit).
\end{itemize}

This hardware configuration allows for testing three distinct parallelism phases:
\begin{enumerate}
    \item \textbf{Physical Scaling (1--10 threads)}: Utilizing dedicated physical cores.
    \item \textbf{SMT Scaling (11--20 threads)}: Utilizing hardware threads (Hyper-Threading).
    \item \textbf{Oversubscription (21--40 threads)}: Exceeding hardware capacity to test context-switching overhead.
\end{enumerate}

\section{Project Structure}

The project follows a modular structure to separate core logic, parallel orchestration, and data management:

\begin{verbatim}
project-root/
|-- Makefile                    # Build automation
|-- doc/                        # Report, figures, and LaTeX sources
|-- src/
|   |-- game_of_life.c          # Core logic and I/O
|   |-- game_of_life_sequential.c
|   |-- game_of_life_pthreads.c # Parallel driver
|   `-- include/game_of_life.h
|-- samples/                    # Benchmark inputs (.txt)
`-- output/                     # Simulation results
\end{verbatim}

\section{Experimental Design}

\subsection{Workloads}
To stress the system, we generated synthetic benchmark datasets with a random cell density of 20\% ($\rho = 0.2$). The grid sizes chosen for evaluation are:
\begin{itemize}
    \item $1000 \times 1000$ (1 million cells)
    \item $2000 \times 2000$ (4 million cells)
    \item $4000 \times 4000$ (16 million cells)
    \item $5000 \times 5000$ (25 million cells)
\end{itemize}
All tests run for 50 generations.

\subsection{Metrics}
We evaluate performance using the following metrics:
\begin{itemize}
    \item \textbf{Execution Time ($T$)}: Wall-clock time of the computation phase, excluding I/O.
    \item \textbf{Speedup ($S$)}: Defined as $S_p = T_{seq} / T_{par}(p)$.
    \item \textbf{Peak Memory (RSS)}: The maximum Resident Set Size of the process, measured via \texttt{getrusage(RUSAGE\_SELF).ru\_maxrss}. RSS captures the physical memory actually touched by the program: both grids (current and next), heap allocations, and the stack pages actually used by all threads (unused portions of the default \textasciitilde8\,MB pthread stacks are not counted). Shared libraries and binary sections are also part of RSS.
\end{itemize}

\subsection{Interpreting RSS for Sequential vs. Parallel Runs}
Sequential and parallel runs report similar RSS because Linux commits stack pages lazily. Each worker thread touches only a few kilobytes of stack (no deep recursion and minimal locals), so the dominant footprint is:
\[
    2 \times (\text{rows} \times \text{cols}) \text{ bytes (double-buffered grids)} + \text{libc + binary} + \text{small stacks}.
\]
Thus, increasing thread count does not significantly inflate peak physical memory.

\subsection{Validation Strategy}
Correctness is validated by simulating standard patterns. We visualize the evolution of a "Glider" (a simple spaceship) and a "Gosper Glider Gun" (a complex oscillator) to ensure that the parallel domain decomposition does not introduce artifacts at thread boundaries.